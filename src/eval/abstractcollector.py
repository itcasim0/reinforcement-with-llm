import arxiv
import numpy as np
from collections import Counter
import re
import json
import time
from typing import List, Dict

class LargeScaleAnalyzer:
    """
    500ê°œ ë…¼ë¬¸ ëŒ€ê·œëª¨ ë¶„ì„ê¸°
    - ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì—ì„œ ìµœì‹  ë…¼ë¬¸ ìˆ˜ì§‘
    - í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ í‰ê°€ ê¸°ì¤€ ìƒì„±
    """
    
    def __init__(self):
        self.papers = []
        self.client = arxiv.Client()
    
    def collect_papers_large_scale(self, target_count=500):
        """
        ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì—ì„œ 500ê°œ ë…¼ë¬¸ ìˆ˜ì§‘
        """
        print(f"\nğŸ“š {target_count}ê°œ ë…¼ë¬¸ ëŒ€ê·œëª¨ ìˆ˜ì§‘ ì‹œì‘...")
        print("   ì¹´í…Œê³ ë¦¬: AI, ML, CV, NLP, Robotics ë“±\n")
        
        papers = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ë¹„ìœ¨
        categories = {
            'cs.LG': 150,  # Machine Learning
            'cs.CV': 150,  # Computer Vision
            'cs.CL': 100,  # NLP
            'cs.AI': 50,   # AI General
            'cs.RO': 30,   # Robotics
            'cs.NE': 20,   # Neural Computing
        }
        
        for category, count in categories.items():
            print(f"ğŸ“ {category} - ëª©í‘œ {count}ê°œ")
            
            try:
                # ìµœê·¼ 2ë…„ ë…¼ë¬¸ (2023-2025)
                search = arxiv.Search(
                    query=f"cat:{category}",
                    max_results=count,
                    sort_by=arxiv.SortCriterion.SubmittedDate,
                    sort_order=arxiv.SortOrder.Descending
                )
                
                collected = 0
                for paper in self.client.results(search):
                    # Abstract ê¸¸ì´ í•„í„° (ë„ˆë¬´ ì§§ì€ ê²ƒ ì œì™¸)
                    if len(paper.summary.split()) < 50:
                        continue
                    
                    paper_info = {
                        'title': paper.title,
                        'abstract': paper.summary,
                        'year': paper.published.year,
                        'arxiv_id': paper.entry_id.split('/')[-1],
                        'authors': [a.name for a in paper.authors[:3]],
                        'category': category,
                        'type': 'recent_paper'
                    }
                    
                    papers.append(paper_info)
                    collected += 1
                    
                    if collected % 20 == 0:
                        print(f"   ì§„í–‰: {collected}/{count}ê°œ")
                    
                    if collected >= count:
                        break
                    
                    time.sleep(0.1)  # Rate limit ë°©ì§€
                
                print(f"   âœ“ ì™„ë£Œ: {collected}ê°œ ìˆ˜ì§‘\n")
                
            except Exception as e:
                print(f"   âœ— ì˜¤ë¥˜: {type(e).__name__}\n")
        
        self.papers = papers
        
        print(f"\nâœ… ì´ {len(papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"   ì—°ë„ ë²”ìœ„: {min(p['year'] for p in papers)} ~ {max(p['year'] for p in papers)}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_counts = {}
        for p in papers:
            cat = p['category']
            category_counts[cat] = category_counts.get(cat, 0) + 1
        
        print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for cat, cnt in sorted(category_counts.items()):
            print(f"   {cat}: {cnt}ê°œ")
        
        return papers
    
    def analyze_abstracts_comprehensive(self):
        """
        500ê°œ ë…¼ë¬¸ ì¢…í•© ë¶„ì„
        """
        print("\n" + "="*80)
        print("ğŸ“Š ëŒ€ê·œëª¨ ë…¼ë¬¸ ì´ˆë¡ ì¢…í•© ë¶„ì„")
        print("="*80)
        
        if not self.papers:
            print("âŒ ë¶„ì„í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        abstracts = [p['abstract'] for p in self.papers]
        
        analysis = {
            'metadata': {
                'total_papers': len(self.papers),
                'categories': list(set(p['category'] for p in self.papers)),
                'year_range': [min(p['year'] for p in self.papers), max(p['year'] for p in self.papers)]
            },
            'basic_stats': self._analyze_basic_stats(abstracts),
            'length_distribution': self._analyze_length_distribution(abstracts),
            'structure_patterns': self._analyze_structure_patterns(abstracts),
            'linguistic_features': self._analyze_linguistic_features(abstracts),
            'content_patterns': self._analyze_content_patterns(abstracts),
            'keyword_analysis': self._analyze_keywords(abstracts),
            'sentence_analysis': self._analyze_sentences(abstracts),
            'advanced_metrics': self._analyze_advanced_metrics(abstracts),
            'category_comparison': self._analyze_by_category()
        }
        
        self._print_comprehensive_analysis(analysis)
        
        return analysis
    
    def _analyze_basic_stats(self, abstracts):
        """ê¸°ë³¸ í†µê³„ (500ê°œ ê¸°ì¤€)"""
        word_counts = [len(abs.split()) for abs in abstracts]
        char_counts = [len(abs) for abs in abstracts]
        
        return {
            'total_papers': len(abstracts),
            'word_count': {
                'mean': float(np.mean(word_counts)),
                'std': float(np.std(word_counts)),
                'min': int(np.min(word_counts)),
                'max': int(np.max(word_counts)),
                'median': float(np.median(word_counts)),
                'q25': float(np.percentile(word_counts, 25)),
                'q50': float(np.percentile(word_counts, 50)),
                'q75': float(np.percentile(word_counts, 75)),
                'q90': float(np.percentile(word_counts, 90)),
                'q10': float(np.percentile(word_counts, 10))
            },
            'char_count': {
                'mean': float(np.mean(char_counts)),
                'median': float(np.median(char_counts))
            }
        }
    
    def _analyze_length_distribution(self, abstracts):
        """ê¸¸ì´ ë¶„í¬ (ë” ì„¸ë¶„í™”)"""
        word_counts = [len(abs.split()) for abs in abstracts]
        
        bins = {
            'very_short': sum(1 for w in word_counts if w < 100),
            'short': sum(1 for w in word_counts if 100 <= w < 150),
            'medium': sum(1 for w in word_counts if 150 <= w < 200),
            'long': sum(1 for w in word_counts if 200 <= w < 250),
            'very_long': sum(1 for w in word_counts if w >= 250)
        }
        
        total = len(abstracts)
        distribution = {k: float(v / total) for k, v in bins.items()}
        
        # íˆìŠ¤í† ê·¸ë¨ ë°ì´í„° ì¶”ê°€
        hist, bin_edges = np.histogram(word_counts, bins=20)
        distribution['histogram'] = {
            'counts': hist.tolist(),
            'bin_edges': bin_edges.tolist()
        }
        
        return distribution
    
    def _analyze_structure_patterns(self, abstracts):
        """êµ¬ì¡° íŒ¨í„´ (500ê°œ ë¶„ì„)"""
        patterns = {
            'has_background': 0,
            'has_problem': 0,
            'has_method': 0,
            'has_results': 0,
            'has_conclusion': 0,
            'full_structure': 0,
            'starts_with_context': 0,
            'ends_with_impact': 0,
        }
        
        background_kw = ['existing', 'current', 'previous', 'traditional', 'conventional']
        problem_kw = ['however', 'challenge', 'difficult', 'limitation', 'problem']
        method_kw = ['we propose', 'we present', 'we introduce', 'our approach', 'our method', 'we develop', 'our model']
        result_kw = ['achieve', 'outperform', 'demonstrate', 'show that', 'significantly', 'improvement', 'better']
        conclusion_kw = ['therefore', 'thus', 'overall', 'in summary', 'our work']
        
        for abstract in abstracts:
            sentences = [s.strip() for s in re.split(r'[.!?]+', abstract) if s.strip()]
            if len(sentences) < 3:
                continue
            
            abs_lower = abstract.lower()
            first_sent = sentences[0].lower()
            last_sent = sentences[-1].lower()
            
            has_bg = any(kw in abs_lower for kw in background_kw)
            has_prob = any(kw in abs_lower for kw in problem_kw)
            has_meth = any(kw in abs_lower for kw in method_kw)
            has_res = any(kw in abs_lower for kw in result_kw)
            has_conc = any(kw in abs_lower for kw in conclusion_kw)
            
            if has_bg:
                patterns['has_background'] += 1
            if has_prob:
                patterns['has_problem'] += 1
            if has_meth:
                patterns['has_method'] += 1
            if has_res:
                patterns['has_results'] += 1
            if has_conc:
                patterns['has_conclusion'] += 1
            if has_bg and has_prob and has_meth and has_res:
                patterns['full_structure'] += 1
            
            if any(kw in first_sent for kw in background_kw + ['recent', 'many', 'in']):
                patterns['starts_with_context'] += 1
            
            if any(kw in last_sent for kw in result_kw + conclusion_kw):
                patterns['ends_with_impact'] += 1
        
        total = len(abstracts)
        return {k: float(v / total) for k, v in patterns.items()}
    
    def _analyze_linguistic_features(self, abstracts):
        """ì–¸ì–´í•™ì  íŠ¹ì§• (500ê°œ)"""
        features = {
            'has_numbers': 0,
            'has_percentage': 0,
            'has_equation': 0,
            'has_comparison': 0,
            'uses_we': 0,
            'uses_our': 0,
            'uses_this_paper': 0,
            'passive_voice': 0,
            'active_voice': 0,
            'has_parentheses': 0,
            'has_hyphen': 0,
            'has_colon': 0,
            'avg_commas': 0,
        }
        
        total_commas = 0
        
        for abstract in abstracts:
            abs_lower = abstract.lower()
            
            if re.search(r'\d+', abstract):
                features['has_numbers'] += 1
            if '%' in abstract or 'percent' in abs_lower:
                features['has_percentage'] += 1
            if any(x in abstract for x in ['$', '\\', 'equation']):
                features['has_equation'] += 1
            if any(w in abs_lower for w in ['better than', 'compared to', 'outperform', 'superior', 'vs']):
                features['has_comparison'] += 1
            if ' we ' in abs_lower or abs_lower.startswith('we '):
                features['uses_we'] += 1
            if ' our ' in abs_lower:
                features['uses_our'] += 1
            if 'this paper' in abs_lower or 'this work' in abs_lower:
                features['uses_this_paper'] += 1
            if any(w in abs_lower for w in [' is ', ' are ', ' was ', ' were ']):
                features['passive_voice'] += 1
            if any(w in abs_lower for w in [' we ', ' they ', ' it ']):
                features['active_voice'] += 1
            if '(' in abstract:
                features['has_parentheses'] += 1
            if '-' in abstract:
                features['has_hyphen'] += 1
            if ':' in abstract:
                features['has_colon'] += 1
            
            total_commas += abstract.count(',')
        
        total = len(abstracts)
        result = {k: float(v / total) for k, v in features.items()}
        result['avg_commas'] = float(total_commas / total)
        
        return result
    
    def _analyze_content_patterns(self, abstracts):
        """ë‚´ìš© íŒ¨í„´ (500ê°œ)"""
        patterns = {
            'mentions_sota': 0,
            'mentions_dataset': 0,
            'mentions_benchmark': 0,
            'mentions_architecture': 0,
            'mentions_model': 0,
            'mentions_novel': 0,
            'mentions_evaluation': 0,
            'mentions_training': 0,
            'mentions_performance': 0,
            'mentions_efficiency': 0,
            'mentions_scalability': 0,
            'mentions_real_world': 0,
        }
        
        datasets = ['imagenet', 'coco', 'mnist', 'cifar', 'glue', 'squad', 'dataset']
        
        for abstract in abstracts:
            abs_lower = abstract.lower()
            
            if 'state-of-the-art' in abs_lower or 'sota' in abs_lower:
                patterns['mentions_sota'] += 1
            if any(ds in abs_lower for ds in datasets):
                patterns['mentions_dataset'] += 1
            if 'benchmark' in abs_lower:
                patterns['mentions_benchmark'] += 1
            if 'architecture' in abs_lower:
                patterns['mentions_architecture'] += 1
            if 'model' in abs_lower:
                patterns['mentions_model'] += 1
            if 'novel' in abs_lower:
                patterns['mentions_novel'] += 1
            if any(w in abs_lower for w in ['evaluation', 'experiment', 'evaluate']):
                patterns['mentions_evaluation'] += 1
            if 'training' in abs_lower or 'train' in abs_lower:
                patterns['mentions_training'] += 1
            if 'performance' in abs_lower or 'accuracy' in abs_lower:
                patterns['mentions_performance'] += 1
            if 'efficient' in abs_lower or 'efficiency' in abs_lower:
                patterns['mentions_efficiency'] += 1
            if 'scalable' in abs_lower or 'scalability' in abs_lower:
                patterns['mentions_scalability'] += 1
            if 'real-world' in abs_lower or 'practical' in abs_lower:
                patterns['mentions_real_world'] += 1
        
        total = len(abstracts)
        return {k: float(v / total) for k, v in patterns.items()}
    
    def _analyze_keywords(self, abstracts):
        """í‚¤ì›Œë“œ ë¹ˆë„ (500ê°œ - ìƒìœ„ 50ê°œ)"""
        all_words = []
        for abstract in abstracts:
            words = re.findall(r'\b[a-z]{4,}\b', abstract.lower())
            all_words.extend(words)
        
        stopwords = {'that', 'this', 'with', 'from', 'have', 'been', 'which', 'their', 
                    'these', 'such', 'than', 'also', 'more', 'other', 'into', 'only',
                    'over', 'very', 'when', 'them', 'about', 'both', 'most', 'many',
                    'where', 'while', 'then', 'there', 'here', 'each', 'some'}
        filtered = [w for w in all_words if w not in stopwords]
        
        counter = Counter(filtered)
        return dict(counter.most_common(50))
    
    def _analyze_sentences(self, abstracts):
        """ë¬¸ì¥ ë¶„ì„ (500ê°œ)"""
        all_sentence_lengths = []
        sentence_counts = []
        first_sentence_lengths = []
        last_sentence_lengths = []
        
        for abstract in abstracts:
            sentences = [s.strip() for s in re.split(r'[.!?]+', abstract) if s.strip()]
            sentence_counts.append(len(sentences))
            
            for i, sent in enumerate(sentences):
                words = len(sent.split())
                all_sentence_lengths.append(words)
                
                if i == 0:
                    first_sentence_lengths.append(words)
                if i == len(sentences) - 1:
                    last_sentence_lengths.append(words)
        
        return {
            'avg_sentences_per_abstract': float(np.mean(sentence_counts)),
            'median_sentences_per_abstract': float(np.median(sentence_counts)),
            'avg_words_per_sentence': float(np.mean(all_sentence_lengths)),
            'median_words_per_sentence': float(np.median(all_sentence_lengths)),
            'sentence_length_std': float(np.std(all_sentence_lengths)),
            'first_sentence_avg': float(np.mean(first_sentence_lengths)),
            'last_sentence_avg': float(np.mean(last_sentence_lengths)),
        }
    
    def _analyze_advanced_metrics(self, abstracts):
        """ê³ ê¸‰ ì§€í‘œ"""
        metrics = {
            'lexical_diversity': [],
            'technical_term_density': [],
            'readability_scores': []
        }
        
        for abstract in abstracts:
            words = abstract.lower().split()
            unique_words = set(words)
            
            # Lexical Diversity (Type-Token Ratio)
            if len(words) > 0:
                ttx = len(unique_words) / len(words)
                metrics['lexical_diversity'].append(ttx)
            
            # Technical Term Density (ê¸´ ë‹¨ì–´ ë¹„ìœ¨)
            technical = sum(1 for w in words if len(w) >= 8)
            if len(words) > 0:
                metrics['technical_term_density'].append(technical / len(words))
            
            # ê°„ë‹¨í•œ ê°€ë…ì„± ì ìˆ˜
            sentences = [s.strip() for s in re.split(r'[.!?]+', abstract) if s.strip()]
            if len(sentences) > 0 and len(words) > 0:
                avg_sent_len = len(words) / len(sentences)
                long_words = sum(1 for w in words if len(w) >= 7)
                readability = avg_sent_len + (long_words / len(words) * 100)
                metrics['readability_scores'].append(readability)
        
        return {
            'avg_lexical_diversity': float(np.mean(metrics['lexical_diversity'])),
            'avg_technical_density': float(np.mean(metrics['technical_term_density'])),
            'avg_readability': float(np.mean(metrics['readability_scores']))
        }
    
    def _analyze_by_category(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ ë¶„ì„"""
        category_stats = {}
        
        for category in set(p['category'] for p in self.papers):
            cat_papers = [p for p in self.papers if p['category'] == category]
            cat_abstracts = [p['abstract'] for p in cat_papers]
            
            if not cat_abstracts:
                continue
            
            word_counts = [len(abs.split()) for abs in cat_abstracts]
            
            category_stats[category] = {
                'count': len(cat_papers),
                'avg_length': float(np.mean(word_counts)),
                'median_length': float(np.median(word_counts))
            }
        
        return category_stats
    
    def _print_comprehensive_analysis(self, analysis):
        """ì¢…í•© ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“ˆ ê¸°ë³¸ í†µê³„ (500ê°œ ë…¼ë¬¸)")
        print("="*80)
        
        stats = analysis['basic_stats']
        print(f"ì´ ë…¼ë¬¸: {stats['total_papers']}ê°œ")
        print(f"\në‹¨ì–´ ìˆ˜ í†µê³„:")
        print(f"  í‰ê· : {stats['word_count']['mean']:.1f} Â± {stats['word_count']['std']:.1f}")
        print(f"  ì¤‘ì•™ê°’: {stats['word_count']['median']:.1f}")
        print(f"  ë²”ìœ„: {stats['word_count']['min']} ~ {stats['word_count']['max']}")
        print(f"  Q10-Q90: {stats['word_count']['q10']:.0f} ~ {stats['word_count']['q90']:.0f}")
        print(f"  Q25-Q75: {stats['word_count']['q25']:.0f} ~ {stats['word_count']['q75']:.0f}")
        print(f"\nğŸ‘‰ ê¶Œì¥ ê¸¸ì´: {stats['word_count']['q25']:.0f}-{stats['word_count']['q75']:.0f} ë‹¨ì–´")
        print(f"ğŸ‘‰ ìµœì  ê¸¸ì´: {stats['word_count']['median']:.0f} ë‹¨ì–´")
        
        print("\n" + "="*80)
        print("ğŸ“Š ê¸¸ì´ ë¶„í¬")
        print("="*80)
        dist = analysis['length_distribution']
        print(f"  ë§¤ìš° ì§§ìŒ (<100):     {dist['very_short']*100:5.1f}%  {'â–ˆ' * int(dist['very_short']*50)}")
        print(f"  ì§§ìŒ (100-150):       {dist['short']*100:5.1f}%  {'â–ˆ' * int(dist['short']*50)}")
        print(f"  ì¤‘ê°„ (150-200):       {dist['medium']*100:5.1f}%  {'â–ˆ' * int(dist['medium']*50)}")
        print(f"  ê¹€ (200-250):         {dist['long']*100:5.1f}%  {'â–ˆ' * int(dist['long']*50)}")
        print(f"  ë§¤ìš° ê¹€ (250+):       {dist['very_long']*100:5.1f}%  {'â–ˆ' * int(dist['very_long']*50)}")
        
        print("\n" + "="*80)
        print("ğŸ—ï¸  êµ¬ì¡° íŒ¨í„´ (500ê°œ ë¶„ì„)")
        print("="*80)
        struct = analysis['structure_patterns']
        print(f"ë°°ê²½/ë§¥ë½:             {struct['has_background']*100:5.1f}%")
        print(f"ë¬¸ì œ ì •ì˜:             {struct['has_problem']*100:5.1f}%")
        print(f"ë°©ë²•ë¡  ì œì‹œ:           {struct['has_method']*100:5.1f}%")
        print(f"ê²°ê³¼ ì œì‹œ:             {struct['has_results']*100:5.1f}%")
        print(f"ê²°ë¡ :                  {struct['has_conclusion']*100:5.1f}%")
        print(f"ì™„ì „í•œ êµ¬ì¡°:           {struct['full_structure']*100:5.1f}%")
        
        print("\n" + "="*80)
        print("âœï¸  ì–¸ì–´ íŠ¹ì§• (500ê°œ)")
        print("="*80)
        ling = analysis['linguistic_features']
        print(f"ìˆ«ì í¬í•¨:             {ling['has_numbers']*100:5.1f}%")
        print(f"ë°±ë¶„ìœ¨ ì‚¬ìš©:           {ling['has_percentage']*100:5.1f}%")
        print(f"ë¹„êµ í‘œí˜„:             {ling['has_comparison']*100:5.1f}%")
        print(f"'We' ì‚¬ìš©:             {ling['uses_we']*100:5.1f}%")
        print(f"'Our' ì‚¬ìš©:            {ling['uses_our']*100:5.1f}%")
        print(f"'This paper' ì‚¬ìš©:     {ling['uses_this_paper']*100:5.1f}%")
        print(f"í‰ê·  ì‰¼í‘œ ê°œìˆ˜:        {ling['avg_commas']:.1f}ê°œ")
        
        print("\n" + "="*80)
        print("ğŸ“ ë‚´ìš© íŒ¨í„´ (500ê°œ)")
        print("="*80)
        content = analysis['content_patterns']
        print(f"SOTA ì–¸ê¸‰:             {content['mentions_sota']*100:5.1f}%")
        print(f"ë°ì´í„°ì…‹:              {content['mentions_dataset']*100:5.1f}%")
        print(f"ë²¤ì¹˜ë§ˆí¬:              {content['mentions_benchmark']*100:5.1f}%")
        print(f"ëª¨ë¸ ì–¸ê¸‰:             {content['mentions_model']*100:5.1f}%")
        print(f"Novel ê°•ì¡°:            {content['mentions_novel']*100:5.1f}%")
        print(f"í‰ê°€/ì‹¤í—˜:             {content['mentions_evaluation']*100:5.1f}%")
        print(f"ì„±ëŠ¥:                  {content['mentions_performance']*100:5.1f}%")
        print(f"íš¨ìœ¨ì„±:                {content['mentions_efficiency']*100:5.1f}%")
        
        print("\n" + "="*80)
        print("ğŸ“„ ë¬¸ì¥ ë¶„ì„")
        print("="*80)
        sent = analysis['sentence_analysis']
        print(f"ì´ˆë¡ë‹¹ í‰ê·  ë¬¸ì¥:      {sent['avg_sentences_per_abstract']:.1f}ê°œ")
        print(f"ë¬¸ì¥ë‹¹ í‰ê·  ë‹¨ì–´:      {sent['avg_words_per_sentence']:.1f}ê°œ")
        print(f"ë¬¸ì¥ë‹¹ ì¤‘ì•™ê°’:         {sent['median_words_per_sentence']:.1f}ê°œ")
        print(f"ì²« ë¬¸ì¥ í‰ê· :          {sent['first_sentence_avg']:.1f}ê°œ")
        print(f"ë§ˆì§€ë§‰ ë¬¸ì¥ í‰ê· :      {sent['last_sentence_avg']:.1f}ê°œ")
        
        print("\n" + "="*80)
        print("ğŸ”¬ ê³ ê¸‰ ì§€í‘œ")
        print("="*80)
        adv = analysis['advanced_metrics']
        print(f"ì–´íœ˜ ë‹¤ì–‘ì„± (TTR):     {adv['avg_lexical_diversity']:.3f}")
        print(f"ì „ë¬¸ ìš©ì–´ ë°€ë„:        {adv['avg_technical_density']:.3f}")
        print(f"ê°€ë…ì„± ì ìˆ˜:           {adv['avg_readability']:.1f}")
        
        print("\n" + "="*80)
        print("ğŸ”¤ ìƒìœ„ 30ê°œ í•µì‹¬ í‚¤ì›Œë“œ")
        print("="*80)
        keywords = analysis['keyword_analysis']
        for i, (word, count) in enumerate(list(keywords.items())[:30], 1):
            if i % 3 == 1:
                print()
            print(f"{word:18s}({count:4d})", end=" ")
        print("\n")
        
        print("="*80)
        print("ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ")
        print("="*80)
        for cat, stats in sorted(analysis['category_comparison'].items()):
            print(f"{cat:10s} | {stats['count']:3d}ê°œ | í‰ê·  {stats['avg_length']:.0f}ë‹¨ì–´")
    
    def save_results(self, filename='large_scale_500_analysis.json'):
        """ê²°ê³¼ ì €ì¥"""
        analysis = self.analyze_abstracts_comprehensive()
        
        if analysis is None:
            return None
        
        output = {
            'metadata': {
                'total_papers': len(self.papers),
                'collection_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'method': 'large_scale_multi_category',
                'categories': list(set(p['category'] for p in self.papers))
            },
            'analysis': analysis,
            'papers_sample': self.papers[:20]  # ìƒ˜í”Œë§Œ ì €ì¥ (íŒŒì¼ í¬ê¸° ê´€ë¦¬)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   - {len(self.papers)}ê°œ ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼")
        print(f"   - í†µê³„ì  í‰ê°€ ê¸°ì¤€ í¬í•¨")
        
        return output
    
    def create_evaluation_model(self, analysis):
        """í†µê³„ ê¸°ë°˜ í‰ê°€ ëª¨ë¸ ìƒì„±"""
        print("\n" + "="*80)
        print("âœ… í‰ê°€ ëª¨ë¸ ìƒì„± (500ê°œ ê¸°ë°˜)")
        print("="*80)
        
        stats = analysis['basic_stats']
        struct = analysis['structure_patterns']
        ling = analysis['linguistic_features']
        content = analysis['content_patterns']
        sent = analysis['sentence_analysis']
        
        model = {
            'scoring_criteria': {
                'length_score': {
                    'weight': 0.15,
                    'optimal_min': stats['word_count']['q25'],
                    'optimal_max': stats['word_count']['q75'],
                    'target': stats['word_count']['median'],
                    'method': 'gaussian_penalty'
                },
                'structure_score': {
                    'weight': 0.30,
                    'has_background': struct['has_background'],
                    'has_method': struct['has_method'],
                    'has_results': struct['has_results'],
                    'full_structure_bonus': 0.2
                },
                'linguistic_score': {
                    'weight': 0.20,
                    'has_numbers': ling['has_numbers'],
                    'has_comparison': ling['has_comparison'],
                    'first_person': max(ling['uses_we'], ling['uses_our']),
                    'target_commas': ling['avg_commas']
                },
                'content_score': {
                    'weight': 0.20,
                    'evaluation': content['mentions_evaluation'],
                    'dataset': content['mentions_dataset'],
                    'performance': content['mentions_performance'],
                    'model': content['mentions_model']
                },
                'sentence_score': {
                    'weight': 0.15,
                    'target_words_per_sentence': sent['avg_words_per_sentence'],
                    'acceptable_range': [
                        sent['avg_words_per_sentence'] - 5,
                        sent['avg_words_per_sentence'] + 5
                    ]
                }
            },
            'thresholds': {
                'excellent': 0.85,
                'good': 0.70,
                'acceptable': 0.55,
                'poor': 0.40
            },
            'statistics': {
                'based_on': len(self.papers),
                'word_count_distribution': {
                    'q10': stats['word_count']['q10'],
                    'q25': stats['word_count']['q25'],
                    'q50': stats['word_count']['q50'],
                    'q75': stats['word_count']['q75'],
                    'q90': stats['word_count']['q90']
                }
            }
        }
        
        print("\nğŸ“Š í‰ê°€ ëª¨ë¸ ìš”ì•½:")
        print(f"\n1. ê¸¸ì´ ì ìˆ˜ (15%):")
        print(f"   ìµœì : {model['scoring_criteria']['length_score']['optimal_min']:.0f}-{model['scoring_criteria']['length_score']['optimal_max']:.0f} ë‹¨ì–´")
        print(f"   ëª©í‘œ: {model['scoring_criteria']['length_score']['target']:.0f} ë‹¨ì–´")
        
        print(f"\n2. êµ¬ì¡° ì ìˆ˜ (30%):")
        print(f"   ë°°ê²½: {struct['has_background']*100:.0f}%")
        print(f"   ë°©ë²•: {struct['has_method']*100:.0f}%")
        print(f"   ê²°ê³¼: {struct['has_results']*100:.0f}%")
        
        print(f"\n3. ì–¸ì–´ ì ìˆ˜ (20%):")
        print(f"   ìˆ«ì: {ling['has_numbers']*100:.0f}%")
        print(f"   ë¹„êµ: {ling['has_comparison']*100:.0f}%")
        
        print(f"\n4. ë‚´ìš© ì ìˆ˜ (20%):")
        print(f"   í‰ê°€: {content['mentions_evaluation']*100:.0f}%")
        print(f"   ì„±ëŠ¥: {content['mentions_performance']*100:.0f}%")
        
        print(f"\n5. ë¬¸ì¥ ì ìˆ˜ (15%):")
        print(f"   ëª©í‘œ: {sent['avg_words_per_sentence']:.1f} ë‹¨ì–´/ë¬¸ì¥")
        
        print(f"\nğŸ¯ ë“±ê¸‰ ê¸°ì¤€:")
        print(f"   Excellent: {model['thresholds']['excellent']*100:.0f}% ì´ìƒ")
        print(f"   Good:      {model['thresholds']['good']*100:.0f}% ì´ìƒ")
        print(f"   Acceptable: {model['thresholds']['acceptable']*100:.0f}% ì´ìƒ")
        
        return model


if __name__ == "__main__":
    print("="*80)
    print("ğŸ¯ 500ê°œ ë…¼ë¬¸ ëŒ€ê·œëª¨ ë¶„ì„ ë° í‰ê°€ ëª¨ë¸ ìƒì„±")
    print("="*80)
    
    analyzer = LargeScaleAnalyzer()
    
    # 500ê°œ ë…¼ë¬¸ ìˆ˜ì§‘
    papers = analyzer.collect_papers_large_scale(target_count=500)
    
    if len(papers) >= 100:  # ìµœì†Œ 100ê°œ ì´ìƒ
        # ë¶„ì„
        output = analyzer.save_results('large_scale_500_analysis.json')
        
        # í‰ê°€ ëª¨ë¸ ìƒì„±
        if output:
            model = analyzer.create_evaluation_model(output['analysis'])
            
            with open('evaluation_model_500.json', 'w', encoding='utf-8') as f:
                json.dump(model, f, indent=2, ensure_ascii=False)
            print("\nğŸ’¾ í‰ê°€ ëª¨ë¸ ì €ì¥: evaluation_model_500.json")
            
            print("\n" + "="*80)
            print("âœ… ì™„ë£Œ!")
            print("="*80)
            print(f"ìˆ˜ì§‘: {len(papers)}ê°œ ë…¼ë¬¸")
            print(f"ë¶„ì„: large_scale_500_analysis.json")
            print(f"ëª¨ë¸: evaluation_model_500.json")
    else:
        print("\nâŒ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ (ìµœì†Œ 100ê°œ í•„ìš”)")