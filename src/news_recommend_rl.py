"""
MIND ë°ì´í„°ì…‹ ê¸°ë°˜ ë‰´ìŠ¤ ì¶”ì²œ ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ (ê°œì„  ë²„ì „)

ê°œì„  ì‚¬í•­:
1. ì‹¤ì œ LLM í†µí•© (CandidateLLM ì‚¬ìš©)
2. Q-Learning ì •ì±… ì¶”ê°€
3. Ground Truth ê¸°ë°˜ ë³´ìƒ ê°œì„ 
4. ë°ì´í„° ê²½ë¡œ ìë™ ê²€ì¦
"""

import os
import pandas as pd
from typing import List, Dict, Tuple, Optional
import random
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from collections import defaultdict
import json

# TODO: ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ import ìˆ˜ì • í•„ìš”
from llm.core import CandidateLLM


# ========== Enums ==========
class TimeSlot(Enum):
    """ì‹œê°„ëŒ€ êµ¬ë¶„"""
    MORNING = "morning"
    LUNCH = "lunch"
    EVENING = "evening"
    NIGHT = "night"


class SummaryLength(Enum):
    """ìš”ì•½ ê¸¸ì´"""
    SHORT = "short"
    MEDIUM = "medium"
    LONG = "long"


# ========== Data Classes ==========
@dataclass
class NewsItem:
    """ê°œë³„ ë‰´ìŠ¤ ì•„ì´í…œ"""
    news_id: str
    category: str
    title: str
    abstract: str


@dataclass
class UserAction:
    """ì‚¬ìš©ìì˜ ì‹¤ì œ ë°˜ì‘ ë°ì´í„°"""
    clicked: bool = False
    read_time: float = 0.0
    shared: bool = False
    liked: bool = False


@dataclass
class Episode:
    """í•œ ì‚¬ìš©ìì˜ ë‰´ìŠ¤ ì†Œë¹„ ì„¸ì…˜"""
    user_id: str
    history: List[str]
    candidates: List[NewsItem]
    timestamp: str
    ground_truth_clicks: List[str]
    
    def get_time_slot(self) -> TimeSlot:
        """íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‹œê°„ëŒ€ë¡œ ë³€í™˜"""
        try:
            dt = datetime.strptime(self.timestamp, "%m/%d/%Y %I:%M:%S %p")
            hour = dt.hour
            
            if 6 <= hour < 12:
                return TimeSlot.MORNING
            elif 12 <= hour < 14:
                return TimeSlot.LUNCH
            elif 14 <= hour < 20:
                return TimeSlot.EVENING
            else:
                return TimeSlot.NIGHT
        except:
            return TimeSlot.MORNING


@dataclass
class MDPState:
    """ê°•í™”í•™ìŠµ State ì •ì˜

    Returns:
        user_history: ì‚¬ìš©ìê°€ ê³¼ê±°ì— ë³¸ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬
        current_time: í˜„ì¬ ì‹œê°„ëŒ€ (ì•„/ì /ì €/ë°¤)
        click_rate: í˜„ì¬ê¹Œì§€ì˜ í´ë¦­ë¥ 
        candidate_categories: ì¶”ì²œ ê°€ëŠ¥í•œ ë‰´ìŠ¤ë“¤ì˜ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
        current_step: í˜„ì¬ ìŠ¤í…, í˜„ì¬ ëª‡ ë²ˆì§¸ ì¶”ì²œì¸ê°€
        max_step: ìµœëŒ€ ìŠ¤í…
        read_completion: ì‚¬ìš©ìê°€ ë‰´ìŠ¤ë¥¼ ëê¹Œì§€ ì½ì„ í™•ë¥ 
        remain_budget: ë‚¨ì€ ì¼ì¼ ì˜ˆì‚° (LLM ìš”ì•½ë•Œë¬¸ì— í•„ìš”)
        last_summary_length: ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ì²œí•œ ë‰´ìŠ¤ ìš”ì•½ ê¸¸ì´
        total_cost: ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©í•œ ì´ ë¹„ìš©
    """
    user_history: List[str]
    current_time: TimeSlot
    click_rate: float
    candidate_categories: List[str]
    current_step: int
    max_step: int
    read_completion: float = 0.7
    remain_budget: float = 10.0 # '$'
    last_summary_length: Optional[SummaryLength] = None
    total_cost: float = 0.0 
    
    # def to_tuple(self) -> tuple:
    #     """ìµœì†Œ state"""
        
    #     # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ í›„ë³´ ë‰´ìŠ¤ ë§¤ì¹­ ê°œìˆ˜ë§Œ
    #     user_interests = set(self.user_history)
    #     matches = sum(1 for cat in self.candidate_categories 
    #                 if cat in user_interests)
    #     match_level = min(matches, 2)  # 0, 1, 2+
        
    #     # ì‹œê°„ëŒ€
    #     time = self.current_time.value
        
    #     return (
    #         match_level,  # 0, 1, 2+ (3ê°€ì§€)
    #         time         # 4ê°€ì§€
    #     )
    def to_tuple(self) -> tuple:
        """Q-Learningì„ ìœ„í•œ í•´ì‹œ ê°€ëŠ¥í•œ state í‘œí˜„ (ê°œì„ )"""
        # ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ (ìµœëŒ€ 3ê°œ ì¹´í…Œê³ ë¦¬)
        # history_str = ','.join(sorted(set(self.user_history[-3:]))) if self.user_history else "none"
        
        # í›„ë³´ ë‰´ìŠ¤ ì¤‘ ê´€ì‹¬ì‚¬ ë§¤ì¹­ ê°œìˆ˜
        user_interests = set(self.user_history)
        interest_matches = sum(1 for cat in self.candidate_categories if cat in user_interests)
        interest_level = min(interest_matches, 3)  # 0, 1, 2, 3, ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ë§¤ì¹­ë„
        
        # í´ë¦­ë¥  (3ë‹¨ê³„ë¡œ ê°„ì†Œí™”)
        if self.click_rate < 0.3:
            click_level = 0  # ë‚®ìŒ
        elif self.click_rate < 0.7:
            click_level = 1  # ì¤‘ê°„
        else:
            click_level = 2  # ë†’ìŒ
        
        # í˜„ì¬ ìŠ¤í… ì¶”ê°€
        step = self.current_step
        
        return (
            # history_str,                  
            self.current_time.value,        
            interest_level,                
            click_level,
            step
        )

def build_llms() -> List:
    """LLM í›„ë³´ ìƒì„±"""

    return [
        CandidateLLM("google/gemini-2.0-flash-001", "ê°€ì„±ë¹„", {"input_price": 0.10, "output_price": 0.40}),
        CandidateLLM("anthropic/claude-sonnet-4", "ê³ í’ˆì§ˆ", {"input_price": 3.0, "output_price": 15.0}),
        CandidateLLM("google/gemma-3-12b-it", "ì˜¤í”ˆì†ŒìŠ¤", {"input_price": 0.04, "output_price": 0.14}),
    ]
        


# ========== MIND Data Loader ==========
class MINDDataLoader:
    """MIND ë°ì´í„°ì…‹ ë¡œë”"""
    
    def __init__(self, data_dir: str):
        self.data_dir = data_dir
        self.news_df = None
        self.behaviors_df = None
    
    def load_data(self):
        """news.tsvì™€ behaviors.tsv ë¡œë“œ"""
        print(f"ğŸ“‚ Loading MIND data from {self.data_dir}...")
        
        news_path = os.path.join(self.data_dir, "news.tsv")
        if os.path.exists(news_path):
            self.news_df = pd.read_csv(
                news_path,
                sep='\t',
                header=None,
                names=['news_id', 'category', 'subcategory', 'title', 'abstract', 
                       'url', 'title_entities', 'abstract_entities']
            )
            print(f"Loaded {len(self.news_df)} news articles")
            print(f"   Categories: {list(self.news_df['category'].value_counts().head(10).items())}")
        else:
            raise FileNotFoundError(f"news.tsv not found in {self.data_dir}")
        
        behaviors_path = os.path.join(self.data_dir, "behaviors.tsv")
        if os.path.exists(behaviors_path):
            self.behaviors_df = pd.read_csv(
                behaviors_path,
                sep='\t',
                header=None,
                names=['impression_id', 'user_id', 'time', 'history', 'impressions']
            )
            print(f"Loaded {len(self.behaviors_df)} behavior logs")
            print(f"   Unique users: {self.behaviors_df['user_id'].nunique()}")
        else:
            raise FileNotFoundError(f"behaviors.tsv not found in {self.data_dir}")
    
    def create_episodes(self, num_episodes: int = 100, min_candidates: int = 3) -> List[Episode]:
        """MIND ë°ì´í„°ë¡œë¶€í„° Episode ìƒì„±"""
        if self.news_df is None or self.behaviors_df is None:
            raise ValueError("ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš” (load_data() í˜¸ì¶œ)")
        
        print(f"\nCreating {num_episodes} episodes from MIND data...")
        
        episodes = []
        sampled_behaviors = self.behaviors_df.sample(min(num_episodes * 2, len(self.behaviors_df)))
        
        for idx, row in sampled_behaviors.iterrows():
            try:
                user_id = row['user_id']
                timestamp = row['time']
                
                # History íŒŒì‹±
                history = []
                if pd.notna(row['history']):
                    history_ids = row['history'].split()
                    for news_id in history_ids[-10:]:
                        news_info = self.news_df[self.news_df['news_id'] == news_id]
                        if not news_info.empty:
                            history.append(news_info.iloc[0]['category'])
                
                # Impressions íŒŒì‹±
                if pd.isna(row['impressions']):
                    continue
                
                impressions = row['impressions'].split()
                candidates = []
                ground_truth_clicks = []
                
                for impression in impressions:
                    parts = impression.rsplit('-', 1)
                    if len(parts) != 2:
                        continue
                    
                    news_id, clicked = parts
                    news_info = self.news_df[self.news_df['news_id'] == news_id]
                    if not news_info.empty:
                        news_row = news_info.iloc[0]
                        candidates.append(NewsItem(
                            news_id=news_id,
                            category=news_row['category'],
                            title=news_row['title'] if pd.notna(news_row['title']) else "No title",
                            abstract=news_row['abstract'] if pd.notna(news_row['abstract']) else "No abstract"
                        ))
                        
                        if clicked == '1':
                            ground_truth_clicks.append(news_id)
                
                if len(candidates) >= min_candidates:
                    if len(candidates) > 8:
                        clicked_news = [c for c in candidates if c.news_id in ground_truth_clicks]
                        not_clicked_news = [c for c in candidates if c.news_id not in ground_truth_clicks]
                        num_not_clicked = min(8 - len(clicked_news), len(not_clicked_news))
                        if num_not_clicked > 0:
                            candidates = clicked_news + random.sample(not_clicked_news, num_not_clicked)
                        else:
                            candidates = clicked_news[:8]
                        random.shuffle(candidates)
                    
                    episode = Episode(
                        user_id=user_id,
                        history=history[-5:] if history else [],
                        candidates=candidates,
                        timestamp=timestamp,
                        ground_truth_clicks=ground_truth_clicks
                    )
                    episodes.append(episode)
                    
                    if len(episodes) >= num_episodes:
                        break
                        
            except Exception as e:
                continue
        
        print(f"Created {len(episodes)} episodes")
        return episodes


# ========== Environment ==========
class NewsRecommendationEnv:
    """ë‰´ìŠ¤ ì¶”ì²œ í™˜ê²½"""
    
    def __init__(self, max_step: int = 5, daily_budget: float = 1.0, alpha: float = 0.05,
                 llms: List = None):
        self.max_step = max_step
        self.daily_budget = daily_budget
        self.alpha = alpha
        self.llm = self._build_single_llm()
        
        # ìš”ì•½ ê¸¸ì´ë³„ ë¹„ìš© (í‰ê· ê°’ ì‚¬ìš©)
        self.summary_costs = {
            SummaryLength.SHORT: 0.03,
            SummaryLength.MEDIUM: 0.08,
            SummaryLength.LONG: 0.15
        }
    
    def _build_single_llm(self):
        """ë‹¨ì¼ LLM ìƒì„± - Mock ë²„ì „"""
        
        class MockLLM:
            def __init__(self):
                self.model = "mock-gemini-flash"
            
            def answer(self, question):
                # ìš”ì•½ ê¸¸ì´ì— ë”°ë¼ ë¹„ìš© ë‹¤ë¥´ê²Œ
                if "ì§§ê²Œ" in question:
                    out_tokens = 30
                    cost = 0.005  # ë§¤ìš° ì‘ì€ ë¹„ìš©
                elif "ìƒì„¸í•˜ê²Œ" in question:
                    out_tokens = 100
                    cost = 0.02
                else:
                    out_tokens = 60
                    cost = 0.01
                
                summary = f"[Mock] ë‰´ìŠ¤ ìš”ì•½"
                return summary, out_tokens, cost, True
        
        return MockLLM()
    # def _build_single_llm(self):
    #     """ë‹¨ì¼ LLM ìƒì„± (gemini-flash ì‚¬ìš©)"""
    #     from llm.core import CandidateLLM
        
    #     return CandidateLLM(
    #         "google/gemini-2.0-flash-001",
    #         "ê°€ì„±ë¹„ ëª¨ë¸",
    #         {"input_price": 0.10, "output_price": 0.40}
    #     )
    
    def reset(self, episode: Episode) -> MDPState:
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.episode = episode
        self.current_step = 0
        self.total_cost = 0.0
        self.action_history = []
        self.user_actions = []
        self.click_history = []
        
        return self._get_state()
    
    def _get_state(self) -> MDPState:
        """í˜„ì¬ state ë°˜í™˜"""
        click_rate = sum(self.click_history) / len(self.click_history) if self.click_history else 0.5
        
        last_summary_length = None
        if self.action_history:
            last_action = self.action_history[-1]
            if last_action[0] == "RECOMMEND" and len(last_action) > 2:
                last_summary_length = last_action[2]
        
        return MDPState(
            user_history=self.episode.history,
            current_time=self.episode.get_time_slot(),
            click_rate=click_rate,
            read_completion=0.7,
            candidate_categories=[n.category for n in self.episode.candidates],
            last_summary_length=last_summary_length,
            total_cost=self.total_cost,
            remain_budget=self.daily_budget - self.total_cost,
            current_step=self.current_step,
            max_step=self.max_step
        )
    
    def step(self, action: Tuple) -> Tuple[MDPState, float, bool]:
        """í–‰ë™ ìˆ˜í–‰ ë° ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        done = False
        
        if action[0] == "STOP":
            done = True
            reward = self._calculate_final_reward()
            self.action_history.append(action)
            
        elif action[0] == "RECOMMEND":
            news_idx = action[1]
            summary_length = action[2] if len(action) > 2 else SummaryLength.MEDIUM
            
            if news_idx >= len(self.episode.candidates):
                reward = -1.0
                done = True
            else:
                news = self.episode.candidates[news_idx]
                
                # ê³ ì •ëœ LLM ì‚¬ìš© (ì„ íƒ ë¶ˆí•„ìš”)
                llm = self.llm
                
                # ìš”ì•½ ê¸¸ì´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
                length_instruction = {
                    SummaryLength.SHORT: "2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ",
                    SummaryLength.MEDIUM: "5-6ë¬¸ì¥ìœ¼ë¡œ",
                    SummaryLength.LONG: "ìƒì„¸í•˜ê²Œ"
                }
                
                question = f"ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ {length_instruction[summary_length]} ìš”ì•½í•´ì£¼ì„¸ìš”.\n\nì œëª©: {news.title}\në‚´ìš©: {news.abstract}"
                
                # LLM API í˜¸ì¶œ
                summary, out_tokens, cost, ok = llm.answer(question)
                
                self.total_cost += cost
                
                # ì‚¬ìš©ì ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜
                user_action = self._simulate_user_action(news, summary_length)
                
                self.user_actions.append(user_action)
                self.click_history.append(1.0 if user_action.clicked else 0.0)
                self.action_history.append((action[0], news_idx, summary_length))
                
                click_reward = 1.0 if user_action.clicked else -0.3
                cost_penalty = -self.alpha * cost
                gt_bonus = 0.5 if news.news_id in self.episode.ground_truth_clicks else 0.0
        
                reward = click_reward + cost_penalty + gt_bonus
                # # ì¦‰ì‹œ ë³´ìƒ: ë¹„ìš© íŒ¨ë„í‹°
                # cost_penalty = -self.alpha * (cost / self.daily_budget)
                # reward = cost_penalty
                
                self.current_step += 1
        
        else:
            reward = -1.0
            done = True
        
        # ìµœëŒ€ ìŠ¤í… ë„ë‹¬
        if self.current_step >= self.max_step and not done:
            done = True
            reward += self._calculate_final_reward()
        
        return self._get_state(), reward, done
    
    def _simulate_user_action(self, news: NewsItem, summary_length: SummaryLength) -> UserAction:
        """
        ë‰´ìŠ¤ ì¶”ì²œì— ëŒ€í•œ ì‚¬ìš©ì ë°˜ì‘ ì‹œë®¬ë ˆì´ì…˜
        """
        is_ground_truth = news.news_id in self.episode.ground_truth_clicks
        
        if is_ground_truth:
            # GTì— ìˆìœ¼ë©´ ë†’ì€ í™•ë¥ ë¡œ í´ë¦­
            clicked = random.random() < 0.9
            read_time = random.uniform(30, 60)
            shared = random.random() < 0.3
        else:
            # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ë§¤ì¹­ í™•ì¸
            user_interests = set(self.episode.history)
            interest_match = news.category in user_interests
            
            click_prob = 0.3 if interest_match else 0.1
            clicked = random.random() < click_prob
            read_time = random.uniform(10, 25) if clicked else 0.0
            shared = random.random() < 0.05 if clicked else False
        
        return UserAction(
            clicked=clicked,
            read_time=read_time,
            shared=shared,
            liked=random.random() < 0.1
        )
    
    def _calculate_outcome_score(self, user_action: UserAction, summary_length: SummaryLength) -> float:
        """ì‚¬ìš©ì ë°˜ì‘ ì ìˆ˜ ê³„ì‚°"""
        click_score = 1.0 if user_action.clicked else 0.0
        
        expected_time = {
            SummaryLength.SHORT: 20,
            SummaryLength.MEDIUM: 35,
            SummaryLength.LONG: 50
        }[summary_length]
        
        read_score = min(user_action.read_time / expected_time, 1.0)
        engagement = 1.0 if user_action.shared else 0.0
        
        outcome = 0.5 * click_score + 0.3 * read_score + 0.2 * engagement
        return outcome
    
    def _calculate_final_reward(self) -> float:
        """ì¢…ë£Œ ì‹œ ìµœì¢… ë³´ìƒ (ë¹„ìš© ì œê±°)"""
        if not self.user_actions:
            return -1.0
        
        # í´ë¦­ë¥  ê³„ì‚°
        clicks = sum(1 for ua in self.user_actions if ua.clicked)
        click_rate = clicks / len(self.user_actions)
        
        # ì½ê¸° ì‹œê°„ ê³„ì‚°
        avg_read_time = sum(ua.read_time for ua in self.user_actions) / len(self.user_actions)
        read_score = min(avg_read_time / 30.0, 1.0)  # 30ì´ˆ ê¸°ì¤€
        
        # ì°¸ì—¬ë„ (ê³µìœ , ì¢‹ì•„ìš”)
        engagement = sum(1 for ua in self.user_actions if ua.shared or ua.liked)
        engagement_score = engagement / len(self.user_actions)
        
        # ìµœì¢… ë³´ìƒ: í´ë¦­ 50% + ì½ê¸° 30% + ì°¸ì—¬ 20%
        final_reward = 0.5 * click_rate + 0.3 * read_score + 0.2 * engagement_score
        
        return final_reward

# ========== Q-Learning Policy ==========
class QLearningPolicy:
    """Q-Learning ì •ì±…
    
    sample:
    Q-table = {
        ('morning', 3, 1, 0): {
            'REC_0_short': 0.88,   # ì´ ìƒí™©ì—ì„œ ë‰´ìŠ¤[0] ì§§ê²Œ ì¶”ì²œ â†’ ì ìˆ˜ 0.88
            'REC_1_medium': 1.32,  # ì´ ìƒí™©ì—ì„œ ë‰´ìŠ¤[1] ì¤‘ê°„ ì¶”ì²œ â†’ ì ìˆ˜ 1.32 ìµœê³ 
            'REC_2_long': 0.65,
            'STOP': -0.87,         # ì´ ìƒí™©ì—ì„œ ì¤‘ë‹¨ â†’ ì ìˆ˜ -0.87 (ë‚˜ì¨)
        },
        ('night', 2, 0, 1): {
            'REC_0_short': 0.45,
            ...
        }
    }
    """
    
    def __init__(self, learning_rate: float = 0.2, discount_factor: float = 0.9, 
                 epsilon: float = 0.8, epsilon_decay: float = 0.96):
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.05
        
        # Q-table: {state: {action: Q-value}}
        self.q_table = defaultdict(lambda: defaultdict(float))
        
        # í•™ìŠµ í†µê³„
        self.training_episodes = 0
    
    def get_action_space(self, state: MDPState) -> List[Tuple]:
        """ ì—ì´ì „íŠ¸ê°€ ê°€ëŠ¥í•œ í–‰ë™ ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ (ë‰´ìŠ¤ ì„ íƒ + ìš”ì•½ ê¸¸ì´)"""
        actions = [("STOP",)]
        
        # ê° í›„ë³´ ë‰´ìŠ¤ Ã— ìš”ì•½ ê¸¸ì´ ì¡°í•©
        for idx in range(len(state.candidate_categories)):
            for length in SummaryLength:  # ìš”ì•½ ê¸¸ì´ ì¶”ê°€
                actions.append(("RECOMMEND", idx, length))
        
        return actions
        
    def act(self, state: MDPState, training: bool = True) -> Tuple:
        """ì•¡ì…˜ ì„ íƒ (epsilon-greedy)"""
        action_space = self.get_action_space(state)
        state_key = state.to_tuple()
        
        # Epsilon-greedy
        if training and random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ ì•¡ì…˜
            return random.choice(action_space)
        else:
            # ì´ìš©: Q-value ìµœëŒ€í™”
            action_values = {
                self._action_to_key(action): self.q_table[state_key][self._action_to_key(action)]
                for action in action_space
            }
            
            if not action_values:
                return random.choice(action_space)
            
            #ëª¨ë“  Q-valueê°€ 0ì´ë©´ (ì²˜ìŒ ë³´ëŠ” state) STOP ì œì™¸í•˜ê³  ëœë¤ ì„ íƒ
            max_q_value = max(action_values.values())
            
            if max_q_value == 0.0:
                # STOP ì œì™¸í•œ RECOMMEND ì•¡ì…˜ë“¤ë§Œ
                non_stop_actions = [a for a in action_space if a[0] != "STOP"]
                if non_stop_actions:
                    return random.choice(non_stop_actions)
            
            best_action_key = max(action_values, key=action_values.get)
            return self._key_to_action(best_action_key)
    
    def update(self, state: MDPState, action: Tuple, reward: float, 
               next_state: MDPState, done: bool):
        """Q-value ì—…ë°ì´íŠ¸"""
        state_key = state.to_tuple()
        action_key = self._action_to_key(action)
        
        current_q = self.q_table[state_key][action_key]
        
        if done:
            target_q = reward
        else:
            next_state_key = next_state.to_tuple()
            next_actions = self.get_action_space(next_state)
            max_next_q = max(
                [self.q_table[next_state_key][self._action_to_key(a)] for a in next_actions],
                default=0.0
            )
            target_q = reward + self.gamma * max_next_q
        
        # Q-value ì—…ë°ì´íŠ¸
        self.q_table[state_key][action_key] = current_q + self.lr * (target_q - current_q)
    
    def decay_epsilon(self):
        """Epsilon ê°ì†Œ"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
    
    def _action_to_key(self, action: Tuple) -> str:
        """ì•¡ì…˜ì„ í•´ì‹œ ê°€ëŠ¥í•œ í‚¤ë¡œ ë³€í™˜"""
        if action[0] == "STOP":
            return "STOP"
        elif action[0] == "RECOMMEND":
            idx = action[1]
            length = action[2].value if len(action) > 2 else "medium"
            return f"REC_{idx}_{length}"
        return str(action)
    
    def _key_to_action(self, key: str) -> Tuple:
        """í‚¤ë¥¼ ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜"""
        if key == "STOP":
            return ("STOP",)
        elif key.startswith("REC_"):
            parts = key.split("_")
            idx = int(parts[1])
            length_str = parts[2]
            length = SummaryLength(length_str)
            return ("RECOMMEND", idx, length)
        return ("STOP",)
    
    def save_model(self, filepath: str):
        """Q-table ì €ì¥"""
        # defaultdictë¥¼ ì¼ë°˜ dictë¡œ ë³€í™˜í•˜ë©´ì„œ íŠœí”Œ í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        q_dict = {}
        for state_key, actions in self.q_table.items():
            # íŠœí”Œ í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            state_str = str(state_key) if isinstance(state_key, tuple) else state_key
            q_dict[state_str] = dict(actions)
        
        model_data = {
            "q_table": q_dict,
            "epsilon": self.epsilon,
            "training_episodes": self.training_episodes
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Q-table saved to {filepath}")
        
    def load_model(self, filepath: str):
        """Q-table ë¡œë“œ"""
        with open(filepath, 'r', encoding='utf-8') as f:
            model_data = json.load(f)
        
        self.q_table = defaultdict(lambda: defaultdict(float))
        for state_str, actions in model_data["q_table"].items():
            # ë¬¸ìì—´ì„ ë‹¤ì‹œ íŠœí”Œë¡œ ë³€í™˜ (eval ì‚¬ìš©)
            try:
                state_key = eval(state_str)
            except:
                state_key = state_str
                
            for action_key, q_value in actions.items():
                self.q_table[state_key][action_key] = q_value
        
        self.epsilon = model_data.get("epsilon", self.epsilon)
        self.training_episodes = model_data.get("training_episodes", 0)
        
        print(f"ğŸ“‚ Q-table loaded from {filepath}")

# ========== Baseline Policies ==========
class GreedyPolicy:
    """íƒìš• ì •ì±… - ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ê¸°ë°˜"""
    
    def __init__(self):
        self.time_preferred_length = {
            TimeSlot.MORNING: SummaryLength.SHORT,
            TimeSlot.LUNCH: SummaryLength.MEDIUM,
            TimeSlot.EVENING: SummaryLength.LONG,
            TimeSlot.NIGHT: SummaryLength.MEDIUM
        }
    
    def act(self, state: MDPState) -> Tuple:
        if state.current_step >= state.max_step - 1:
            return ("STOP",)
        
        # ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ ê°€ì¥ ë§¤ì¹­ë˜ëŠ” ë‰´ìŠ¤ ì„ íƒ
        user_interests = set(state.user_history)
        selected_idx = None
        
        for idx, category in enumerate(state.candidate_categories):
            if category in user_interests:
                selected_idx = idx
                break
        
        if selected_idx is None and state.candidate_categories:
            selected_idx = 0
        
        if selected_idx is not None:
            # ì‹œê°„ëŒ€ì— ë”°ë¥¸ ìš”ì•½ ê¸¸ì´ ì„ íƒ
            preferred_length = self.time_preferred_length[state.current_time]
            
            return ("RECOMMEND", selected_idx, preferred_length)
        
        return ("STOP",)

# ========== Training & Evaluation ==========
def train_q_learning(env: NewsRecommendationEnv, episodes: List[Episode], 
                     num_epochs: int = 30, save_path: str = None):
    """Q-Learning ì •ì±… í•™ìŠµ"""
    policy = QLearningPolicy(learning_rate=0.2, discount_factor=0.9, epsilon=0.8)
    
    print(f"\n{'='*80}")
    print(f"Q-Learning í•™ìŠµ ì‹œì‘ (Epochs: {num_epochs}, Episodes: {len(episodes)})")
    print("=" * 80)
    
    all_rewards = []
    
    for epoch in range(num_epochs):
        epoch_rewards = []
        
        for ep_idx, episode in enumerate(episodes, 1):
            state = env.reset(episode)
            episode_reward = 0.0
            done = False
            
            trajectory = []  # (state, action, reward) ê¸°ë¡
            
            while not done:
                action = policy.act(state, training=True)
                next_state, reward, done = env.step(action)
                
                # Q-value ì—…ë°ì´íŠ¸
                policy.update(state, action, reward, next_state, done)
                
                episode_reward += reward
                trajectory.append((state, action, reward))
                state = next_state
            
            epoch_rewards.append(episode_reward)
            policy.training_episodes += 1
            
            if ep_idx % 20 == 0:
                avg_reward = sum(epoch_rewards[-20:]) / len(epoch_rewards[-20:])
                print(f"  [Epoch {epoch+1}] Ep {ep_idx}/{len(episodes)} | "
                      f"Avg Reward (last 20): {avg_reward:.3f} | "
                      f"Epsilon: {policy.epsilon:.3f}")
        
        policy.decay_epsilon()
        all_rewards.extend(epoch_rewards)
        
        avg_epoch_reward = sum(epoch_rewards) / len(epoch_rewards)
        print(f"\n  Epoch {epoch+1} ì™„ë£Œ | Avg Reward: {avg_epoch_reward:.3f}\n")
    
    if save_path:
        policy.save_model(save_path)
    
    print("\n í•™ìŠµëœ Q-table í†µê³„:")
    print(f"   ì´ state ìˆ˜: {len(policy.q_table)}")
    print(f"   í‰ê·  ë°©ë¬¸ íšŸìˆ˜: {1000 / len(policy.q_table):.1f}íšŒ")

    # ìƒ˜í”Œ Q-value ì¶œë ¥
    sample_states = list(policy.q_table.keys())[:5]
    for state in sample_states:
        print(f"\n   State: {state}")
        actions = policy.q_table[state]
        top_3 = sorted(actions.items(), key=lambda x: x[1], reverse=True)[:3]
        for action, q in top_3:
            print(f"     {action}: Q={q:.4f}")
    
    return policy


def evaluate_policy(env: NewsRecommendationEnv, episodes: List[Episode], 
                    policy, policy_name: str):
    """ì •ì±… í‰ê°€"""
    print(f"\n{'='*80}")
    print(f"ì •ì±… í‰ê°€: {policy_name}")
    print("=" * 80)
    
    total_rewards = []
    total_costs = []
    total_clicks = []
    total_accuracy = []
    
    for ep_idx, episode in enumerate(episodes, 1):
        state = env.reset(episode)
        episode_reward = 0.0
        done = False
        
        # ì²« ì—í”¼ì†Œë“œ ë””ë²„ê¹…
        if ep_idx == 1:
            print(f"\nì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ ë””ë²„ê¹…:")
            print(f"   ì´ˆê¸° state - step: {state.current_step}, max_step: {state.max_step}")
            print(f"   candidates: {len(state.candidate_categories)}ê°œ")
        
        step_count = 0
        while not done:
            # Q-Learningì¼ ê²½ìš° training=Falseë¡œ ì„¤ì •
            if isinstance(policy, QLearningPolicy):
                action = policy.act(state, training=False)
            else:
                action = policy.act(state)
            
            # ì²« ì—í”¼ì†Œë“œì˜ ì²« ì•¡ì…˜ í™•ì¸
            if ep_idx == 1 and step_count == 0:
                print(f"   ì²« ì•¡ì…˜: {action}")
            
            next_state, reward, done = env.step(action)
            episode_reward += reward
            state = next_state
            step_count += 1
            
            # ë¬´í•œë£¨í”„ ë°©ì§€
            if step_count > 10:
                print(f" ìŠ¤í…ì´ 10íšŒ ì´ˆê³¼, ê°•ì œ ì¢…ë£Œ")
                break
        
        # ì²« ì—í”¼ì†Œë“œ ê²°ê³¼
        if ep_idx == 1:
            print(f"   ì´ ìŠ¤í…: {step_count}")
            print(f"   ì¶”ì²œ ìˆ˜: {len(env.user_actions)}")
            print(f"   ì—í”¼ì†Œë“œ ë³´ìƒ: {episode_reward:.3f}")
        
        # í†µê³„ ìˆ˜ì§‘
        clicks = sum(1 for ua in env.user_actions if ua.clicked)
        total_recs = len(env.user_actions)
        
        recommended_ids = [episode.candidates[ah[1]].news_id for ah in env.action_history if ah[0] == "RECOMMEND"]
        correct_recs = sum(1 for news_id in recommended_ids if news_id in episode.ground_truth_clicks)
        accuracy = correct_recs / len(episode.ground_truth_clicks) if episode.ground_truth_clicks else 0.0
        
        total_rewards.append(episode_reward)
        total_costs.append(env.total_cost)
        if total_recs > 0:
            total_clicks.append(clicks / total_recs)
        else:
            total_clicks.append(0.0)
        total_accuracy.append(accuracy)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n[{policy_name} ì •ì±… ì¢…í•© ê²°ê³¼]")
    print(f"  í‰ê·  ë³´ìƒ: {sum(total_rewards)/len(total_rewards):.3f}")
    print(f"  í‰ê·  ë¹„ìš©: ${sum(total_costs)/len(total_costs):.2f}")
    if total_clicks:
        print(f"  í‰ê·  í´ë¦­ë¥ : {sum(total_clicks)/len(total_clicks)*100:.1f}%")
    else:
        print(f"  í‰ê·  í´ë¦­ë¥ : 0.0% (ì¶”ì²œ ì—†ìŒ)")
    print(f"  í‰ê·  GT Accuracy: {sum(total_accuracy)/len(total_accuracy)*100:.1f}%")
    print("=" * 80)
    
    return {
        "avg_reward": sum(total_rewards)/len(total_rewards),
        "avg_cost": sum(total_costs)/len(total_costs),
        "avg_click_rate": sum(total_clicks)/len(total_clicks) if total_clicks else 0.0,
        "avg_accuracy": sum(total_accuracy)/len(total_accuracy)
    }

# ========== Q-Learning ì˜ì‚¬ê²°ì • ì‹œê°í™” ==========
def visualize_q_learning_decision(policy: QLearningPolicy, env: NewsRecommendationEnv, 
                                   episode: Episode, verbose: bool = True):
    """
    Q-Learning ì •ì±…ì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ ìƒì„¸íˆ ì¶œë ¥
    
    Args:
        policy: í•™ìŠµëœ Q-Learning ì •ì±…
        env: í™˜ê²½
        episode: í…ŒìŠ¤íŠ¸í•  ì—í”¼ì†Œë“œ
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    """
    print("\n" + "="*80)
    print(" Q-Learning ì˜ì‚¬ê²°ì • ê³¼ì • ë¶„ì„")
    print("="*80)
    
    # ì—í”¼ì†Œë“œ ì •ë³´
    print(f"\n ì—í”¼ì†Œë“œ ì •ë³´:")
    print(f"   ì‚¬ìš©ì ID: {episode.user_id}")
    print(f"   ì‹œê°„: {episode.timestamp} ({episode.get_time_slot().value})")
    print(f"   ì‚¬ìš©ì íˆìŠ¤í† ë¦¬: {episode.history}")
    print(f"   Ground Truth í´ë¦­: {episode.ground_truth_clicks}")
    
    print(f"\n í›„ë³´ ë‰´ìŠ¤ ëª©ë¡:")
    for idx, news in enumerate(episode.candidates):
        gt_mark = "â­" if news.news_id in episode.ground_truth_clicks else ""
        print(f"   [{idx}] {news.category:15s} | {news.title[:50]}... {gt_mark}")
    
    # í™˜ê²½ ì´ˆê¸°í™”
    state = env.reset(episode)
    done = False
    step_num = 0
    
    while not done and step_num < 10:
        step_num += 1
        print(f"\n{'â”€'*80}")
        print(f" Step {step_num}")
        print(f"{'â”€'*80}")
        
        # í˜„ì¬ state ì •ë³´
        print(f"\n í˜„ì¬ State:")
        print(f"   user_history: {state.user_history}")
        print(f"   current_time: {state.current_time.value}")
        print(f"   click_rate: {state.click_rate:.2f}")
        print(f"   current_step: {state.current_step}/{state.max_step}")
        print(f"   total_cost: ${state.total_cost:.2f}")
        
        # Stateë¥¼ Q-table keyë¡œ ë³€í™˜
        state_key = state.to_tuple()
        print(f"\n State Key (Q-table ê²€ìƒ‰ìš©):")
        print(f"   {state_key}")
        
        # ê°€ëŠ¥í•œ ì•¡ì…˜ë“¤ê³¼ Q-value í™•ì¸
        action_space = policy.get_action_space(state)
        print(f"\n ê°€ëŠ¥í•œ ì•¡ì…˜ê³¼ Q-value:")
        
        action_q_values = []
        for action in action_space[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
            action_key = policy._action_to_key(action)
            q_value = policy.q_table[state_key][action_key]
            action_q_values.append((action, q_value))
            
            if action[0] == "STOP":
                print(f"   STOP                              Q={q_value:.4f}")
            else:
                news_idx = action[1]
                length = action[2].value
                category = state.candidate_categories[news_idx]
                print(f"   REC[{news_idx}] {category:12s} {length:6s}  Q={q_value:.4f}")
        
        if len(action_space) > 10:
            print(f"   ... (ì´ {len(action_space)}ê°œ ì•¡ì…˜ ì¤‘ 10ê°œë§Œ í‘œì‹œ)")
        
        # ìµœê³  Q-value ì°¾ê¸°
        max_q = max([q for _, q in action_q_values])
        best_actions = [a for a, q in action_q_values if q == max_q]
        
        print(f"\n ìµœê³  Q-value: {max_q:.4f}")
        print(f"   í›„ë³´ ì•¡ì…˜ ìˆ˜: {len(best_actions)}ê°œ")
        
        # Q-Learning ì •ì±…ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ
        selected_action = policy.act(state, training=False)
        
        print(f"\n ì„ íƒëœ ì•¡ì…˜:")
        if selected_action[0] == "STOP":
            print(f"   STOP")
        else:
            news_idx = selected_action[1]
            length = selected_action[2].value
            news = episode.candidates[news_idx]
            gt_mark = "(GT í´ë¦­!)" if news.news_id in episode.ground_truth_clicks else ""
            
            print(f"   ë‰´ìŠ¤ ì¸ë±ìŠ¤: {news_idx}")
            print(f"   ì¹´í…Œê³ ë¦¬: {news.category}")
            print(f"   ìš”ì•½ ê¸¸ì´: {length}")
            print(f"   ì œëª©: {news.title[:60]}...")
            print(f"   {gt_mark}")
            
            # ì„ íƒ ì´ìœ  ë¶„ì„
            print(f"\nì„ íƒ ì´ìœ :")
            if max_q == 0.0:
                print(f"     ì²˜ìŒ ë³´ëŠ” state (Q-value ëª¨ë‘ 0)")
                print(f"   â†’ STOP ì œì™¸í•˜ê³  ëœë¤ ì„ íƒ")
            elif max_q > 0:
                print(f"    í•™ìŠµëœ ê²½í—˜ í™œìš© (ì–‘ìˆ˜ Q-value)")
                print(f"   â†’ ê³¼ê±°ì— ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ ì•¡ì…˜")
            else:
                print(f"     í•™ìŠµëœ ê²½í—˜ìƒ ì¢‹ì§€ ì•ŠìŒ (ìŒìˆ˜ Q-value)")
                print(f"   â†’ ê·¸ë‚˜ë§ˆ ëœ ë‚˜ìœ ì„ íƒ")
            
            # ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ë§¤ì¹­ í™•ì¸
            user_interests = set(state.user_history)
            if news.category in user_interests:
                print(f"    ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ ì¼ì¹˜! (historyì— '{news.category}' ìˆìŒ)")
            else:
                print(f"     ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ ë¶ˆì¼ì¹˜ (history: {user_interests})")
        
        # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰
        next_state, reward, done = env.step(selected_action)
        
        print(f"\n ê²°ê³¼:")
        print(f"   ì¦‰ì‹œ ë³´ìƒ: {reward:.4f}")
        
        if selected_action[0] == "RECOMMEND":
            user_action = env.user_actions[-1]
            print(f"   ì‚¬ìš©ì ë°˜ì‘:")
            print(f"     - í´ë¦­: {' Yes' if user_action.clicked else ' No'}")
            print(f"     - ì½ê¸° ì‹œê°„: {user_action.read_time:.1f}ì´ˆ")
            print(f"     - ê³µìœ : {' Yes' if user_action.shared else ' No'}")
        
        if done:
            print(f"\n ì—í”¼ì†Œë“œ ì¢…ë£Œ")
            print(f"   ìµœì¢… ë³´ìƒ: {reward:.4f}")
            print(f"   ì´ ë¹„ìš©: ${env.total_cost:.2f}")
            print(f"   ì´ ì¶”ì²œ ìˆ˜: {len(env.user_actions)}")
            
            if env.user_actions:
                clicks = sum(1 for ua in env.user_actions if ua.clicked)
                click_rate = clicks / len(env.user_actions)
                print(f"   í´ë¦­ë¥ : {click_rate*100:.1f}%")
        
        state = next_state
    
    print("\n" + "="*80)


def demo_q_learning_decisions(q_policy: QLearningPolicy, env: NewsRecommendationEnv, 
                               test_episodes: List[Episode], num_demos: int = 3):
    """
    í•™ìŠµëœ Q-Learningì˜ ì˜ì‚¬ê²°ì •ì„ ì—¬ëŸ¬ ì—í”¼ì†Œë“œì—ì„œ ì‹œì—°
    
    Args:
        q_policy: í•™ìŠµëœ Q-Learning ì •ì±…
        env: í™˜ê²½
        test_episodes: í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œë“¤
        num_demos: ì‹œì—°í•  ì—í”¼ì†Œë“œ ìˆ˜
    """
    print("\n" + "="*80)
    print(" Q-Learning ì˜ì‚¬ê²°ì • ì‹œì—°")
    print("="*80)
    
    for i in range(min(num_demos, len(test_episodes))):
        episode = test_episodes[i]
        visualize_q_learning_decision(q_policy, env, episode, verbose=True)
        
        if i < num_demos - 1:
            input("\n ë‹¤ìŒ ì—í”¼ì†Œë“œë¥¼ ë³´ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

# ========== Main ==========
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("="*80)
    print("MIND ë°ì´í„°ì…‹ ê¸°ë°˜ ë‰´ìŠ¤ ì¶”ì²œ ê°•í™”í•™ìŠµ (LLM ê³ ì • ë²„ì „)")
    print("="*80)
    
    DATA_DIR = "src/news_data"
    
    if not os.path.exists(DATA_DIR):
        print(f"\n ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        return
    
    # ë°ì´í„° ë¡œë“œ
    loader = MINDDataLoader(DATA_DIR)
    
    try:
        loader.load_data()
    except FileNotFoundError as e:
        print(f"\n {e}")
        return
    
    # ì—í”¼ì†Œë“œ ìƒì„±
    train_episodes = loader.create_episodes(num_episodes=500, min_candidates=4)
    test_episodes = loader.create_episodes(num_episodes=50, min_candidates=4)

    if not train_episodes or not test_episodes:
        print("\n ì—í”¼ì†Œë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    #  í™˜ê²½ ì´ˆê¸°í™” (LLM íŒŒë¼ë¯¸í„° ì œê±°)
    env = NewsRecommendationEnv(
        max_step=4,
        daily_budget=10.0,  # ì˜ˆì‚° ì¦ê°€
        alpha=0.05            # ë¹„ìš© íŒ¨ë„í‹° ê°ì†Œ
    )
    
    print(f"\nğŸ’¡ ì‚¬ìš© LLM: {env.llm.model}")
    print(f"   ì˜ˆì‚°: ${env.daily_budget}, Alpha: {env.alpha}")
    
    # Q-Learning í•™ìŠµ
    q_policy = train_q_learning(env, train_episodes, num_epochs=30, save_path="q_table.json")
    
    #  ì˜ì‚¬ê²°ì • ê³¼ì • ì‹œì—° ì¶”ê°€
    print("\n" + "="*80)
    print(" í•™ìŠµëœ Q-Learningì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì„ í™•ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("="*80)
    
    demo_q_learning_decisions(q_policy, env, test_episodes, num_demos=3)

    # ë² ì´ìŠ¤ë¼ì¸ ì •ì±…
    greedy_policy = GreedyPolicy()
    
    # í‰ê°€
    results = {}
    results["Q-Learning"] = evaluate_policy(env, test_episodes, q_policy, "Q-Learning")
    results["Greedy"] = evaluate_policy(env, test_episodes, greedy_policy, "Greedy")
    
    # ë¹„êµ
    print(f"\n{'='*80}")
    print("ì •ì±… ë¹„êµ")
    print("=" * 80)
    for policy_name, metrics in results.items():
        print(f"\n{policy_name}:")
        for metric_name, value in metrics.items():
            if "cost" in metric_name:
                print(f"  {metric_name}: ${value:.3f}")
            elif "rate" in metric_name or "accuracy" in metric_name:
                print(f"  {metric_name}: {value*100:.1f}%")
            else:
                print(f"  {metric_name}: {value:.3f}")

if __name__ == "__main__":
    main()